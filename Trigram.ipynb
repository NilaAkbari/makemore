{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    with open('names.txt', 'r') as f:\n",
    "        names = f.read().splitlines()\n",
    "    return names\n",
    "    \n",
    "names = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating \"stoi\" and \"itos\"\n",
    "chars = sorted(list(set(''.join(names))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [], []\n",
    "\n",
    "for n in names:\n",
    "    chs = ['.', '.'] + list(n) + ['.','.']\n",
    "    for chs1, chs2, chs3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[chs1]\n",
    "        ix2 = stoi[chs2]\n",
    "        ix3 = stoi[chs3]\n",
    "        xs.append((ix1, ix2))\n",
    "        ys.append(ix3)\n",
    "    \n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one-hot encoded of xs\n",
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "xenc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([260179, 2, 27])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.cat([xenc[0][0], xenc[0][1]])\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc_cat = xenc.view(-1, 2*27)\n",
    "xenc_cat[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing W\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((2*27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260179"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = xenc.shape[0]\n",
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  2.2992334365844727\n",
      "loss:  2.295475482940674\n",
      "loss:  2.2918198108673096\n",
      "loss:  2.2882637977600098\n",
      "loss:  2.2848024368286133\n",
      "loss:  2.28143310546875\n",
      "loss:  2.2781527042388916\n",
      "loss:  2.2749581336975098\n",
      "loss:  2.271845579147339\n",
      "loss:  2.268812417984009\n",
      "loss:  2.265856981277466\n",
      "loss:  2.2629752159118652\n",
      "loss:  2.2601656913757324\n",
      "loss:  2.25742506980896\n",
      "loss:  2.254751682281494\n",
      "loss:  2.252143383026123\n",
      "loss:  2.2495977878570557\n",
      "loss:  2.24711275100708\n",
      "loss:  2.2446866035461426\n",
      "loss:  2.2423171997070312\n",
      "loss:  2.2400026321411133\n",
      "loss:  2.237741470336914\n",
      "loss:  2.2355315685272217\n",
      "loss:  2.2333714962005615\n",
      "loss:  2.23125958442688\n",
      "loss:  2.2291946411132812\n",
      "loss:  2.2271745204925537\n",
      "loss:  2.225198268890381\n",
      "loss:  2.2232649326324463\n",
      "loss:  2.221372365951538\n",
      "loss:  2.21951961517334\n",
      "loss:  2.217705488204956\n",
      "loss:  2.215928316116333\n",
      "loss:  2.2141876220703125\n",
      "loss:  2.212482213973999\n",
      "loss:  2.210810899734497\n",
      "loss:  2.209172487258911\n",
      "loss:  2.207566261291504\n",
      "loss:  2.205991268157959\n",
      "loss:  2.204446315765381\n",
      "loss:  2.202930450439453\n",
      "loss:  2.2014429569244385\n",
      "loss:  2.1999833583831787\n",
      "loss:  2.1985507011413574\n",
      "loss:  2.197144031524658\n",
      "loss:  2.195762872695923\n",
      "loss:  2.194406032562256\n",
      "loss:  2.193073272705078\n",
      "loss:  2.1917641162872314\n",
      "loss:  2.190477132797241\n",
      "loss:  2.1892123222351074\n",
      "loss:  2.187969207763672\n",
      "loss:  2.186746835708618\n",
      "loss:  2.185544967651367\n",
      "loss:  2.1843628883361816\n",
      "loss:  2.1832001209259033\n",
      "loss:  2.182056188583374\n",
      "loss:  2.1809306144714355\n",
      "loss:  2.179823160171509\n",
      "loss:  2.1787328720092773\n",
      "loss:  2.177659749984741\n",
      "loss:  2.176603078842163\n",
      "loss:  2.175563335418701\n",
      "loss:  2.174539089202881\n",
      "loss:  2.173530101776123\n",
      "loss:  2.172536611557007\n",
      "loss:  2.1715574264526367\n",
      "loss:  2.170593023300171\n",
      "loss:  2.169642925262451\n",
      "loss:  2.1687064170837402\n",
      "loss:  2.167783498764038\n",
      "loss:  2.1668736934661865\n",
      "loss:  2.1659767627716064\n",
      "loss:  2.165092706680298\n",
      "loss:  2.1642208099365234\n",
      "loss:  2.1633613109588623\n",
      "loss:  2.162513017654419\n",
      "loss:  2.161677122116089\n",
      "loss:  2.1608521938323975\n",
      "loss:  2.160038471221924\n",
      "loss:  2.159235715866089\n",
      "loss:  2.1584436893463135\n",
      "loss:  2.1576621532440186\n",
      "loss:  2.156891107559204\n",
      "loss:  2.1561295986175537\n",
      "loss:  2.1553783416748047\n",
      "loss:  2.154636859893799\n",
      "loss:  2.153904438018799\n",
      "loss:  2.153181791305542\n",
      "loss:  2.152468204498291\n",
      "loss:  2.1517632007598877\n",
      "loss:  2.1510677337646484\n",
      "loss:  2.1503803730010986\n",
      "loss:  2.1497015953063965\n",
      "loss:  2.149031162261963\n",
      "loss:  2.1483688354492188\n",
      "loss:  2.147714376449585\n",
      "loss:  2.1470682621002197\n",
      "loss:  2.1464295387268066\n",
      "loss:  2.145798444747925\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "\n",
    "    logits = xenc_cat @ W #equivalent to count's logarithm\n",
    "    counts = logits.exp()\n",
    "    probs = counts/counts.sum(1, keepdim=True)\n",
    "    loss = -probs[torch.arange(num), ys].log().mean()\n",
    "    print(\"loss: \", loss.item())\n",
    "\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    #update\n",
    "    W.data += -10 * W.grad\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
