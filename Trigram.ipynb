{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    with open('names.txt', 'r') as f:\n",
    "        names = f.read().splitlines()\n",
    "    return names\n",
    "    \n",
    "names = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating \"stoi\" and \"itos\"\n",
    "chars = sorted(list(set(''.join(names))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [], []\n",
    "\n",
    "for n in names:\n",
    "    chs = ['.', '.'] + list(n) + ['.','.']\n",
    "    for chs1, chs2, chs3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[chs1]\n",
    "        ix2 = stoi[chs2]\n",
    "        ix3 = stoi[chs3]\n",
    "        xs.append((ix1, ix2))\n",
    "        ys.append(ix3)\n",
    "    \n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one-hot encoded of xs\n",
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "xenc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([260179, 2, 27])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.cat([xenc[0][0], xenc[0][1]])\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc_cat = xenc.view(-1, 2*27)\n",
    "xenc_cat[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing W\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((2*27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260179"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = xenc.shape[0]\n",
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  2.084049940109253\n",
      "loss:  2.0836710929870605\n",
      "loss:  2.0832998752593994\n",
      "loss:  2.0829362869262695\n",
      "loss:  2.0825793743133545\n",
      "loss:  2.0822293758392334\n",
      "loss:  2.0818865299224854\n",
      "loss:  2.081549644470215\n",
      "loss:  2.081219434738159\n",
      "loss:  2.080894947052002\n",
      "loss:  2.0805766582489014\n",
      "loss:  2.08026385307312\n",
      "loss:  2.0799572467803955\n",
      "loss:  2.079655647277832\n",
      "loss:  2.079359292984009\n",
      "loss:  2.079068422317505\n",
      "loss:  2.078782558441162\n",
      "loss:  2.0785017013549805\n",
      "loss:  2.0782251358032227\n",
      "loss:  2.077953577041626\n",
      "loss:  2.077686309814453\n",
      "loss:  2.0774238109588623\n",
      "loss:  2.0771656036376953\n",
      "loss:  2.076911449432373\n",
      "loss:  2.0766613483428955\n",
      "loss:  2.076415538787842\n",
      "loss:  2.0761735439300537\n",
      "loss:  2.075934886932373\n",
      "loss:  2.075700521469116\n",
      "loss:  2.075469732284546\n",
      "loss:  2.075242280960083\n",
      "loss:  2.0750181674957275\n",
      "loss:  2.0747976303100586\n",
      "loss:  2.074580430984497\n",
      "loss:  2.074366331100464\n",
      "loss:  2.074155330657959\n",
      "loss:  2.0739476680755615\n",
      "loss:  2.0737428665161133\n",
      "loss:  2.0735411643981934\n",
      "loss:  2.0733423233032227\n",
      "loss:  2.073146104812622\n",
      "loss:  2.0729527473449707\n",
      "loss:  2.0727622509002686\n",
      "loss:  2.0725741386413574\n",
      "loss:  2.0723888874053955\n",
      "loss:  2.0722060203552246\n",
      "loss:  2.0720255374908447\n",
      "loss:  2.071847677230835\n",
      "loss:  2.071672201156616\n",
      "loss:  2.0714991092681885\n",
      "loss:  2.0713284015655518\n",
      "loss:  2.071159601211548\n",
      "loss:  2.070993185043335\n",
      "loss:  2.070828676223755\n",
      "loss:  2.070666790008545\n",
      "loss:  2.0705068111419678\n",
      "loss:  2.0703487396240234\n",
      "loss:  2.070192575454712\n",
      "loss:  2.0700385570526123\n",
      "loss:  2.0698862075805664\n",
      "loss:  2.0697362422943115\n",
      "loss:  2.0695877075195312\n",
      "loss:  2.069441080093384\n",
      "loss:  2.069296360015869\n",
      "loss:  2.069153070449829\n",
      "loss:  2.069011926651001\n",
      "loss:  2.0688722133636475\n",
      "loss:  2.0687341690063477\n",
      "loss:  2.0685980319976807\n",
      "loss:  2.068463087081909\n",
      "loss:  2.0683300495147705\n",
      "loss:  2.0681984424591064\n",
      "loss:  2.068068265914917\n",
      "loss:  2.0679397583007812\n",
      "loss:  2.067812442779541\n",
      "loss:  2.0676870346069336\n",
      "loss:  2.0675625801086426\n",
      "loss:  2.067439556121826\n",
      "loss:  2.0673182010650635\n",
      "loss:  2.0671982765197754\n",
      "loss:  2.0670793056488037\n",
      "loss:  2.0669617652893066\n",
      "loss:  2.066845655441284\n",
      "loss:  2.066730499267578\n",
      "loss:  2.0666167736053467\n",
      "loss:  2.06650447845459\n",
      "loss:  2.0663931369781494\n",
      "loss:  2.0662832260131836\n",
      "loss:  2.066174268722534\n",
      "loss:  2.0660665035247803\n",
      "loss:  2.065959930419922\n",
      "loss:  2.065854549407959\n",
      "loss:  2.0657498836517334\n",
      "loss:  2.0656468868255615\n",
      "loss:  2.065544605255127\n",
      "loss:  2.065443515777588\n",
      "loss:  2.065343141555786\n",
      "loss:  2.065244197845459\n",
      "loss:  2.065145969390869\n",
      "loss:  2.0650486946105957\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "\n",
    "    logits = xenc_cat @ W #equivalent to count's logarithm\n",
    "    counts = logits.exp()\n",
    "    probs = counts/counts.sum(1, keepdim=True)\n",
    "    loss = -probs[torch.arange(num), ys].log().mean()\n",
    "    print(\"loss: \", loss.item())\n",
    "\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    #update\n",
    "    W.data += -40 * W.grad\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-8.5252e+00,  2.8615e+00,  3.5079e-01,  ..., -1.1317e+00,\n",
      "          1.2614e+00,  8.2836e-01],\n",
      "        [ 1.5217e+00,  4.5951e-01, -4.2785e-01,  ..., -1.3107e+00,\n",
      "         -6.9945e-03, -4.4299e-01],\n",
      "        [ 4.5454e-01,  9.9407e-01,  9.9703e-02,  ..., -6.1054e-01,\n",
      "          3.2660e-01, -1.1959e+00],\n",
      "        ...,\n",
      "        [ 3.6765e-01, -6.6534e-02, -1.6148e-01,  ..., -4.1084e-01,\n",
      "          2.2709e-02,  1.4128e-01],\n",
      "        [ 1.3108e+00,  1.9231e+00, -3.6624e-01,  ..., -1.2010e+00,\n",
      "         -9.7226e-01,  1.0099e-01],\n",
      "        [-5.7171e-02,  1.9434e+00,  1.2649e-01,  ..., -1.4299e-01,\n",
      "          8.7116e-01, -3.8587e-01]], requires_grad=True) <built-in method type of Tensor object at 0x0000023BDC1482D0>\n"
     ]
    }
   ],
   "source": [
    "print(W, W.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([54, 27])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "juwide\n",
      "janasad\n",
      "pariay\n",
      "ainn\n",
      "koi\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "    ix1 = 0 \n",
    "    ix2 = 0 #because we begin our words from .. which is index 0\n",
    "    out = []\n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([ix1, ix2]), num_classes=27).float().reshape(-1)\n",
    "        logits = xenc @ W\n",
    "        p = logits.exp()/(logits.exp().sum(-1, keepdim=True))\n",
    "        ix1 = ix2\n",
    "        ix2 = torch.multinomial(p, num_samples=1, replacement=True, generator=g ).item()\n",
    "        if ix2 == 0:\n",
    "            break\n",
    "        out.append(itos[ix2])\n",
    "\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['k', 'o', 'i']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
